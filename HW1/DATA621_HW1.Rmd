---
title: "DATA 621 Homework #1"
author: "Calvin, Juanelle, Kevin, Ravi, Sudhan"
date: "9/25/2019"
output: 
    html_document:
        toc: TRUE
        toc_float: TRUE
        theme: lumen
        number_sections: TRUE
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Exploration

Load and do initial review of the data.  Need to address missing data.

```{r message = FALSE}
library(tidyverse)
library(funModeling)

gh <- "https://raw.githubusercontent.com/kecbenson/DATA621/master/HW1/"
file_train <- paste0(gh, "moneyball-training-data.csv")
file_test <- paste0(gh, "moneyball-evaluation-data.csv")

df_train <- read_csv(file_train)
df_test <- read_csv(file_test)

head(df_train)
str(df_train)
summary(df_train)
attach(df_train)
```

Check distributions of the data; skewness may need to be dealt with transformations. 

```{r cache = TRUE}
hist(df_train$TARGET_WINS)
#pairs(df_train)

for (j in 2:ncol(df_train)) {
    hist(df_train[[j]], main = paste0("Histogram of ", colnames(df_train)[j]),
         xlab = colnames(df_train)[j], freq = FALSE)
    minval <- min(df_train[[j]], na.rm = TRUE)
    maxval <- max(df_train[[j]], na.rm = TRUE)
    meanval <- mean(df_train[[j]], na.rm = TRUE)
    sdval <- sd(df_train[[j]], na.rm = TRUE)
    grid <- minval:maxval
    lines(grid, dnorm(grid, mean = meanval, sd = sdval), lty = 3)
}
```

Review pair-wise relationships between predictor variables and target variable.

```{r cache = TRUE}
# batting variables (hits through home runs)
pairs(df_train[2:6])
# batting variables (walks, strikeouts, hit by pitch)
pairs(df_train[c(2, 7:8, 11)])
# baserun and fielding variables
pairs(df_train[c(2, 9:10, 16:17)])
# pitching variables
pairs(df_train[c(2, 12:15)])
```


# Data Preparation

Data preparation is a pre-processing step that involves cleansing, transforming, and consolidating data. Our first step in the data preparation process was to identify what variables needed to be manipulated within the dataset. We utilize the funModeling package for this purpose. This package contains a set of functions related to exploratory data analysis, data preparation, and model performance. funModeling is intimately related to the Data Science Live Book -Open Source- (2017). Here, funModeling df_status(), is being used to  analyze the zeros, missing values (NA), infinity, data type, and number of unique values for a given dataset.

```{r}
# df_status function to show zero's and missing values
train_data_status <- df_status(df_train, print_results=FALSE) 
# order by percentage of missing values
train_data_status[order(-train_data_status$p_na),]
```

With this particular dataset, using the df_status(), we identified our biggest challenge was to deal with NA's. There was a minimal amount of zero's which accounted for less than one percent of the dataset for any variable. Therefore, we decided to focus on NA's. We ordered the percentage of NA's and identified these variables to be transformed using different imputation methods to be discussed below. 

TEAM_BATTING_HBP	
TEAM_BASERUN_CS	
TEAM_FIELDING_DP	
TEAM_BASERUN_SB	
TEAM_BATTING_SO
TEAM_PITCHING_SO

We decided to use mean imputation on all variables, however, for variables above ten percent (10%) threshold we included the use of a dummy variable to identify if an NA is present. The three variables with a dummy variable are TEAM_BATTING_HBP, TEAM_BASERUN_CS, and TEAM_FIELDING_DP.

Creating a dummy variable called HBP_missing which triggers "1"" if TEAM_BATTING_HBP value is NA and "0" if not. Imputing mean value on all NA's.
```{r}
# trigger a dummy variable if NA is present
df_train$HBP_missing <- ifelse(is.na(df_train$TEAM_BATTING_HBP), 1, 0)
# imputing mean value as a replacement to NA
df_train$TEAM_BATTING_HBP[is.na(df_train$TEAM_BATTING_HBP)] <- mean(df_train$TEAM_BATTING_HBP, na.rm=TRUE)
```

Creating a dummy variable called CS_missing which triggers "1" if TEAM_BASERUN_CS value is NA and "0" if not. Imputing mean value on all NA's.
```{r}
# trigger a dummy variable if NA is present
df_train$CS_missing <- ifelse(is.na(df_train$TEAM_BASERUN_CS), 1, 0)
# imputing mean value as a replacement to NA
df_train$TEAM_BASERUN_CS[is.na(df_train$TEAM_BASERUN_CS)] <- mean(df_train$TEAM_BASERUN_CS, na.rm=TRUE)
```

Creating a dummy variable called DP_missing which triggers "1" if TEAM_FIELDING_DP value is NA and "0" if not. Imputing mean value on all NA's.
```{r}
# trigger a dummy variable if NA is present
df_train$DP_missing <- ifelse(is.na(df_train$TEAM_FIELDING_DP), 1, 0)
# imputing mean value as a replacement to NA
df_train$TEAM_FIELDING_DP[is.na(df_train$TEAM_FIELDING_DP)] <- mean(df_train$TEAM_FIELDING_DP, na.rm=TRUE)
```

These variables fall under the ten percent (10%) threshold. No dummy variables will be utilized, mean imputation will be used for all NA's.
```{r}
# imputing mean value as a replacement to NA
df_train$TEAM_BASERUN_SB[is.na(df_train$TEAM_BASERUN_SB)] <- mean(df_train$TEAM_BASERUN_SB, na.rm=TRUE)
# imputing mean value as a replacement to NA
df_train$TEAM_BATTING_SO[is.na(df_train$TEAM_BATTING_SO)] <- mean(df_train$TEAM_BATTING_SO, na.rm=TRUE)
# imputing mean value as a replacement to NA
df_train$TEAM_PITCHING_SO[is.na(df_train$TEAM_PITCHING_SO)] <- mean(df_train$TEAM_PITCHING_SO, na.rm=TRUE)
```

We can see that all NA's are addressed after imputation methods.
```{r}
# results after imputation, we see all NA's are addressed
df_status(df_train)
```

Our next step is to deal with outliers identified within the data distribution section. We identified four variables which needed to be worked. 

TEAM_PITCHING_SO
TEAM_PITCHING_H
TEAM_PITCHING_BB
TEAM_FIELDING_E

We identified through boxplots which variables are impacted with outliers. Secondly, TEAM_FIELDING_E shows the potential of applying transformation to retain all data points.
```{r}
# reviewing outliers
boxplot(df_train$TEAM_PITCHING_SO)
# reviewing outliers
boxplot(df_train$TEAM_PITCHING_H)
# reviewing outliers
boxplot(df_train$TEAM_PITCHING_BB)
# determining opportunity for transformation
hist(df_train$TEAM_FIELDING_E)
```

Based on John Tukey box-and-whisker plot to display IQR values, he picked 1.5Ã—IQR as the demarkation line for outliers. We will retain this approach and remove outliers for TEAM_PITCHING_SO, TEAM_PITCHING_H, and TEAM_PITCHING_BB. We removed a total of 285 records which accounts for approximately 12.5% of the total dataset. 

We then boxploted these variables again to demonstrate normalization.
```{r}
# assign the TEAM_PITCHING_SO outliers into a vector
outliers_SO <- boxplot(df_train$TEAM_PITCHING_SO, plot=FALSE)$out
# removing TEAM_PITCHING_SO outliers
df_train <- df_train[-which(df_train$TEAM_PITCHING_SO %in% outliers_SO),]
# demonstrating outliers removed, compared to above
boxplot(df_train$TEAM_PITCHING_SO)

# assign the TEAM_PITCHING_H outlier values into a vector
outliers_H <- boxplot(df_train$TEAM_PITCHING_H, plot=FALSE)$out
# removing TEAM_PITCHING_H outliers
df_train <- df_train[-which(df_train$TEAM_PITCHING_H %in% outliers_H),]
# demonstrating outliers removed, compared to above
boxplot(df_train$TEAM_PITCHING_H)

# assign the TEAM_PITCHING_BB outlier values into a vector
outliers_BB <- boxplot(df_train$TEAM_PITCHING_BB, plot=FALSE)$out
# removing TEAM_PITCHING_BB outliers
df_train <- df_train[-which(df_train$TEAM_PITCHING_BB %in% outliers_BB),]
# demonstrating outliers removed, compared to above
boxplot(df_train$TEAM_PITCHING_BB)
```

For the fourth variable, TEAM_FIELDING_E, we transformed the data using log 10 transformation. The data is still skewed, however, shows much improvement from the original variable. We deduct that this transformation is adequate and will retain this method of transformation.
```{r}
# performing log 10 transformation
df_train$TEAM_FIELDING_E = log10(df_train$TEAM_FIELDING_E)
# determining distribution of data
hist(df_train$TEAM_FIELDING_E)
```

We are now ready to proceed with regression model building.

# Model Building

Sample models - note that I haven't dealt with the missing values yet, so stats below are not right.

Suggest organizing the models we show:
- Kitchen sink model: use all variables at once
- Do step-by-step variable elimination: at each step, drop the variable that is not signif; stop when all variables are signif at 5%
- Try models with transformed variables: (a) try log or sqrt transform for skewed variables; (b) try aggregating related variables

```{r}

lm0 <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_BATTING_HBP + 
              TEAM_BASERUN_SB + TEAM_BASERUN_CS)
(lm0sum <- summary(lm0))

lm1 <- lm(TARGET_WINS ~ I(TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_BATTING_HBP) + 
              I(TEAM_BASERUN_SB - TEAM_BASERUN_CS))
(lm1sum <- summary(lm1))

lm2 <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_BASERUN_SB + 
              TEAM_FIELDING_E + TEAM_PITCHING_H + TEAM_PITCHING_BB + TEAM_PITCHING_SO)
(lm2sum <- summary(lm2))

lm3 <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_2B + TEAM_BATTING_3B +
              TEAM_BATTING_HR + TEAM_BATTING_BB + TEAM_BATTING_SO + TEAM_BATTING_HBP)
(lm3sum <- summary(lm3))

lm3a <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_HR + TEAM_BATTING_BB + 
               TEAM_BATTING_SO)
(lm3asum <- summary(lm3a))

lm4 <- lm(TARGET_WINS ~ TEAM_BASERUN_SB + TEAM_BASERUN_CS + TEAM_FIELDING_E +
              TEAM_FIELDING_DP)
(lm4sum <- summary(lm4))

lm5 <- lm(TARGET_WINS ~ TEAM_PITCHING_H + TEAM_PITCHING_HR + TEAM_PITCHING_BB +
              TEAM_PITCHING_SO)
(lm5sum <- summary(lm5))

lm6 <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_HR + TEAM_BATTING_BB + 
              TEAM_BATTING_SO + TEAM_BASERUN_SB + TEAM_FIELDING_E + 
              TEAM_PITCHING_H + TEAM_PITCHING_HR + TEAM_PITCHING_BB)
(lm6sum <- summary(lm6))

lm7 <- lm(TARGET_WINS ~ I(TEAM_BATTING_H + TEAM_BATTING_BB) + TEAM_BATTING_HR + 
              I(TEAM_BASERUN_SB - TEAM_BASERUN_CS) + TEAM_FIELDING_E +
              I(TEAM_PITCHING_H + TEAM_PITCHING_BB) + TEAM_PITCHING_HR)
(lm7sum <- summary(lm7))

```


# Model Selection

We start by reviewing summary statistics for the various models.

```{r}
library(knitr)

models <- list(lm0, lm1, lm2, lm3, lm4, lm5, lm6, lm7)
modsums <- list(lm0sum, lm1sum, lm2sum, lm3sum, lm4sum, lm5sum, lm6sum, lm7sum)
nmod <- length(modsums)

nvar <- integer(nmod)
sigma <- numeric(nmod)
rsq <- numeric(nmod)
adj_rsq <- numeric(nmod)
fstat_p <- numeric(nmod)

for (j in 1:nmod) {
    nvar[j] <- modsums[[j]]$df[1]
    sigma[j] <- modsums[[j]]$sigma
    rsq[j] <- modsums[[j]]$r.squared
    adj_rsq[j] <- modsums[[j]]$adj.r.squared
    fstat_p[j] <- 1 - pf(modsums[[j]]$fstatistic[1], modsums[[j]]$fstatistic[2], 
                         modsums[[j]]$fstatistic[3])
}

modnames <- paste0("lm", 0:7)

eval <- data.frame(Model = modnames, 
                   N_Variables = nvar,
                   Resid_Sigma = sigma,
                   R_Squared = rsq,
                   Adj_R_Squared = adj_rsq,
                   F_P_Value = fstat_p)

kable(eval, digits = 3, align = 'c', caption = 'Summary Evaluation Statistics')
```

>>> NEED to include MSE, RMSE 

```{r}
str(df_test)
head(df_test)

```


    
detach(df_train)

```

